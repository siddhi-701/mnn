1A Simple ml program
import numpy as np
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
class SimpleSVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):
        self.lr = learning_rate
        self.lambda_param = lambda_param
        self.n_iters = n_iters
        self.w = None
        self.b = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        y_ = np.where(y <= 0, -1, 1)

        self.w = np.zeros(n_features)
        self.b = 0

        for _ in range(self.n_iters):
            for idx, x_i, in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                if not condition:
                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]

    def predict(self, X):
        approx = np.dot(X, self.w) - self.b
        return np.sign(approx)
X, y = make_blobs(n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=40)
y = np.where(y == 0, -1, 1)
clf = SimpleSVM()
clf.fit(X, y)

def visualize_svm():
    plt.scatter(X[:, 0], X[:, 1], marker="o", c=y)

    x0_1 = np.amin(X[:, 0])
    x0_2 = np.amax(X[:, 0])

    x1_1 = -(clf.w[0] * x0_1 + clf.b) / clf.w[1]
    x1_2 = -(clf.w[0] * x0_2 + clf.b) / clf.w[1]
    plt.plot([x0_1, x0_2], [x1_1, x1_2], "y--")
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.title('SVM Decision Boundary')
    plt.show()
visualize_svm()



1B . Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based on a given set of training data sample



import csv
num_attributes = 5
a = []


with open('content\Data.csv', 'r') as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
        a.append(row)
        print(row)


print("\nThe initial value of hypothesis:")
hypothesis = ['0'] * num_attributes
print(hypothesis)


for j in range(num_attributes):
    hypothesis[j] = a[1][j]
print("\nThe a[1] value of hypothesis:")
print(hypothesis)


print("\nFind S: Finding a Maximally Specific Hypothesis\n")
for i in range(len(a)):
    if a[i][num_attributes] == 'Positive':
        for j in range(num_attributes):
            if a[i][j] != hypothesis[j]:
                hypothesis[j] = '?'
            else:
                hypothesis[j] = a[i][j]


    print("For Training instance No:{} the hypothesis is".format(i), hypothesis)

print("\nThe Maximally Specific Hypothesis for a given Training Examples:\n", hypothesis)


3A Naive Bayesian

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
data = pd.read_csv('content\PlayTennis.csv')
print("The first 5 Values of data is:\n", data.head())
X = data.drop(columns=['Play Tennis'])
y = data['Play Tennis']
label_encoders = {}


for column in X.columns:
    label_encoders[column] = LabelEncoder()
    X[column] = label_encoders[column].fit_transform(X[column])

y = LabelEncoder().fit_transform(y)

print("\nNow the train data is:\n", X.head())
print("\nNow the train output is:\n", y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)
classifier = GaussianNB()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy is:", accuracy)

2A Perform Data Loading, Feature selection (Principal Component Analysis) and Feature Scoring and Ranking.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
dataset = pd.read_csv("C:/Users/Aditi/Downloads/archive (4)/winequality-red.csv")
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
classifier = LogisticRegression(random_state=0, multi_class="multinomial")
classifier.fit(X_train_pca, y_train)
y_pred = classifier.predict(X_test_pca)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
plt.figure(figsize=(10, 6))
for i, j in enumerate(np.unique(y_train)):
    plt.scatter(X_train_pca[y_train == j, 0], X_train_pca[y_train == j, 1], label=str(j))
plt.title("Logistic Regression (Training Set)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.show()
plt.figure(figsize=(10, 6))
for i, j in enumerate(np.unique(y_test)):
    plt.scatter(X_test_pca[y_test == j, 0], X_test_pca[y_test == j, 1], label=str(j))
plt.title("Logistic Regression (Test Set)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.show()

2B For a given set of training data examples stored in .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.

import csv
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx


def g_0(n):
    return ("?",) * n

def s_0(n):
    return ('0',) * n

def more_general(h1, h2):
    return all(x == "?" or (x != "0" and (x == y or y == "0")) for x, y in zip(h1, h2))

def fulfills(example, hypothesis):
    return more_general(hypothesis, example)

def min_generalizations(h, x):
    return [tuple('?' if h_i != '0' and h_i != x_i else h_i for h_i, x_i in zip(h, x))]

def min_specializations(h, domains, x):
    return [tuple('0' if h_i == x_i else h_i for h_i, x_i in zip(h, x)) for h in h if h != '0'] + \
           [tuple(val if h_i == '0' else h_i for h_i, val in zip(h, domains_i)) for h_i, domains_i in zip(h, domains) if h_i == "?"]

with open('C:/Users/Aditi/OneDrive/Documents/season-dataset.csv') as csvFile:
    examples = [tuple(line) for line in csv.reader(csvFile)]

def get_domains(examples):
    return [list(sorted(set(x_i for x in examples))) for x_i in zip(*examples)]

def candidate_elimination(examples):
    domains = get_domains(examples)[:-1]

    G = set([g_0(len(domains))])
    S = set([s_0(len(domains))])

    for x_cx in examples:
        x, cx = x_cx[:-1], x_cx[-1]
        if cx == 'Y':
            G = {g for g in G if fulfills(x, g)}
            S = {s for s in S if not fulfills(x, s)}
            S.update(min_generalizations(s, x) for s in S)
            S = {s for s in S if any([more_general(g, s) for g in G])}
            S -= {s for s in S if any([more_general(s, s1) for s1 in S if s != s1])}
        else:
            S = {s for s in S if not fulfills(x, s)}
            G = {g for g in G if any([fulfills(x, s) for s in S])}
            G.update(min_specializations(g, domains, x) for g in G)
            G -= {g for g in G if any([more_general(g1, g) for g1 in G if g != g1])}
    return G, S

G, S = candidate_elimination(examples)
print("G[4] = ", G)
print("S[4] = ", S)

def build_hypothesis_space(G, S):
    levels = [[HypothesisNode(x, 0) for x in G]]
    curlevel = 1

    def next_level(h, S):
        for s in S:
            for i in range(len(h)):
                if h[i] == "?" and s[i] != "?":
                    yield h[:i] + (s[i],) + h[i + 1:]

    nextLvl = {}

    while True:
        for n in levels[-1]:
            for hyp in next_level(n.h, S):
                if hyp in nextLvl:
                    nextLvl[hyp].parents.add(n)
                else:
                    nextLvl[hyp] = HypothesisNode(hyp, curlevel, [n])
        if not nextLvl:
            break
        levels.append(list(nextLvl.values()))
        curlevel += 1
        nextLvl = {}
    return levels

def draw_hypothesis_space(G, S):
    levels = build_hypothesis_space(G, S)

    g = nx.Graph()

    for nodes in levels:
        for n in nodes:
            for p in n.parents:
                g.add_edge(n.h, p.h)

    pos = {}
    ymin = 0.1
    ymax = 0.9

    for nodes, y in [(levels[0], ymin), (levels[-1], ymax)]:
        xvals = np.linspace(0, 1, len(nodes))
        for x, n in zip(xvals, nodes):
            pos[n.h] = [x, y]

    pos = nx.layout.fruchterman_reingold_layout(g, pos=pos, fixed=pos.keys())

    nx.draw_networkx_edges(g, pos=pos, alpha=0.25)
    nx.draw_networkx_labels(g, pos=pos)

    plt.box(True)
    plt.xticks([])
    plt.yticks([])
    plt.xlim(-1, 2)
    plt.gcf().set_size_inches((10, 10))
    plt.show()

print()
draw_hypothesis_space(G, S)


4A Least square method
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_csv('C:/Users/Aditi/OneDrive/Documents/Sample_Salary_Data.csv')
X = data.iloc[:, 0]
Y = data.iloc[:, 1]
X_mean = np.mean(X)
Y_mean = np.mean(Y)
numerator = np.sum((X - X_mean) * (Y - Y_mean))
denominator = np.sum((X - X_mean) ** 2)
m = numerator / denominator
c = Y_mean - m * X_mean
print("Coefficient m:", m)
print("Intercept c:", c)
Y_pred = m * X + c
plt.scatter(X, Y, color='blue')
plt.plot(X, Y_pred, color='red')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.title('Linear Regression')
plt.show()

4B Logistic Regression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from matplotlib.colors import ListedColormap
dataset = pd.read_csv('C:/Users/Aditi/OneDrive/Documents/p4b.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
classifier = LogisticRegression()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: \n", cm)
print("Accuracy: ", accuracy_score(y_test, y_pred))
plt.figure(figsize=(8, 6))
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_test[y_test == j, 0], X_test[y_test == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
plt.title('Logistic Regression')
plt.xlabel('X1')
plt.ylabel('X2')
plt.legend()
plt.show()


5A decision tree based ID3 algorithm
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
iris = datasets.load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred))
print("Accuracy: ", accuracy_score(y_test, y_pred))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names)
plt.show()

5B Write a program to implement K-Nearest Neighbour algorithm to classify the iris data set.
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn import datasets
from sklearn.model_selection import train_test_split, GridSearchCV
iris = datasets.load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())
param_grid = {
    'kneighborsclassifier__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],
    'kneighborsclassifier__p': [1, 2],
    'kneighborsclassifier__weights': ['uniform', 'distance'],
    'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
}
gs = GridSearchCV(pipeline, param_grid=param_grid,
                  scoring='accuracy',
                  refit=True,
                  cv=10,
                  verbose=1,
                  n_jobs=2)
gs.fit(X_train, y_train)
print('Best Score: %.3f' % gs.best_score_,
      '\nBest Parameters: ', gs.best_params_)


6A . Implement the different Distance methods (Euclidean, Manhattan Distance, Minkowski Distance) with Prediction, Test Score and Confusion Matrix.
from math import sqrt
from sklearn.metrics import confusion_matrix, classification_report
def euclidean_distance(a, b):
    return sqrt(sum((e1-e2) ** 2 for e1, e2 in zip(a, b)))
def manhattan_distance(a, b):
    return sum(abs(e1 - e2) for e1, e2 in zip(a, b))
def minkowski_distance(a, b, p):
    return sum(abs(e1 - e2) ** p for e1, e2 in zip(a, b)) ** (1/p)
actual = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1]
predicted = [1, 0, 0, 1, 0, 0, 0, 1, 0, 0]
dist1 = euclidean_distance(actual, predicted)
print("Euclidean Distance: ", dist1)
dist2 = manhattan_distance(actual, predicted)
print("Manhattan Distance: ", dist2)
dist3 = minkowski_distance(actual, predicted, 1)
print("Minkowski Distance wiht p = 1: ", dist3)
dist3 = minkowski_distance(actual, predicted, 2)
print("Minkowski Distance with p = 2: ", dist3)
matrix = confusion_matrix(actual, predicted, labels=[1, 0])
print("Confusion Matrix: \n", matrix)
tp, fn, fp, tn = confusion_matrix(actual, predicted, labels=[1, 0]).reshape(-1)
print("Outcome values: \n", tp, fn, fp, tn)
matrix = classification_report(actual, predicted, labels=[1, 0])
print("Classificaiton report: \n", matrix)

6B K mean clustering 
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
raw_data = pd.read_csv('C:/Users/Aditi/Downloads/Classified Data.csv', index_col=0)
print(raw_data.head())
print(raw_data.columns)
scaler = StandardScaler()
scaler.fit(raw_data.drop('TARGET CLASS', axis=1))
scaled_features = scaler.transform(raw_data.drop('TARGET CLASS', axis=1))
scaled_data = pd.DataFrame(scaled_features, columns=raw_data.drop('TARGET CLASS', axis=1).columns)
x = scaled_data
y = raw_data['TARGET CLASS']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)
model = KNeighborsClassifier(n_neighbors=1)
model.fit(x_train, y_train)
predictions = model.predict(x_test)
print(classification_report(y_test, predictions))
print(confusion_matrix(y_test, predictions))

7A hierarchical clustering 
import matplotlib.pyplot as plt
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
dataset = pd.read_csv('C:/Users/Aditi/OneDrive/Documents/abalone.csv')
X = dataset.iloc[:, [3, 4]].values
dendrogram = sch.dendrogram(sch.linkage(X, method="ward"))
plt.title("Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Euclidean distances")
plt.show()
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)
print("Cluster Labels: ", y_hc)
colors = ['red', 'blue', 'green', 'cyan', 'magenta']
for cluster_num in range(5):
    plt.scatter(X[y_hc == cluster_num, 0], X[y_hc == cluster_num, 1], s=100, c=colors[cluster_num], label=f'Cluster {cluster_num + 1}')
plt.title("Clusters of Customers (Hierarchical Clustering Model)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()

7B Implement the Rule based method and test the same

import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix
data = {
    'Tid': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Refund': ['yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no'],
    'Marital_Status': ['single', 'married', 'single', 'married', 'divorced', 'married', 'divorced', 'married', 'single', 'married'],
    'Taxable_Income': [125, 100, 70, 120, 65, 70, 200, 85, 75, 90],
    'Class': ['no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes']
}
df = pd.DataFrame(data)
def rule_based_classifier(row):
    if row['Refund'] == 'yes' and row['Marital_Status'] == 'single' and row['Taxable_Income'] > 100:
        return 'yes'
    elif row['Refund'] == 'no' and row['Marital_Status'] == 'married' and row['Taxable_Income'] <= 80:
        return 'yes'
    else:
        return 'no'
df['Prediction'] = df.apply(rule_based_classifier, axis=1)
accuracy = accuracy_score(df['Class'], df['Prediction'])
conf_matrix = confusion_matrix(df['Class'], df['Prediction'])
print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('\nPrediction Values:')
print(df[['Tid', 'Class', 'Prediction']])


8A Bayesian network
import numpy as np
import pandas as pd
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.models import BayesianNetwork
from pgmpy.inference import VariableElimination
import warnings
warnings.filterwarnings('ignore')
heartDisease = pd.read_csv('heart_disease_data.csv')
heartDisease = heartDisease.replace('?', np.nan)
model = BayesianNetwork([('age', 'target'), ('sex', 'target'), ('exang', 'target'), 
                         ('cp', 'target'), ('restecg', 'target'), ('trestbps', 'target'), 
                         ('chol', 'target'), ('fbs', 'target')])
model.fit(heartDisease, estimator=MaximumLikelihoodEstimator)
HeartDisease_infer = VariableElimination(model)
q1 = HeartDisease_infer.query(variables=['target'], evidence={'restecg': 1})
print("Probability of Heart Disease given evidence = restecg:", q1)
q2 = HeartDisease_infer.query(variables=['target'], evidence={'cp': 2})
print("Probability of Heart Disease given evidence = cp:", q2)

8B  Locally Weighted Regression algorithm 

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
data = pd.read_csv('10-dataset.csv')
bill = np.array(data.total_bill)
tip = np.array(data.tip)
def kernel(point, xmat, k):
    m, n = np.shape(xmat)
    weights = np.mat(np.eye((m)))
    for j in range(m):
        diff = point - X[j]
        weights[j, j] = np.exp(diff * diff.T / (-2.0 * k ** 2))
    return weights
def localWeight(point, xmat, ymat, k):
    wei = kernel(point, xmat, k)
    W = (X.T * (wei * X)).I * (X.T * (wei * ymat.T))
    return W
def localWeightRegression(xmat, ymat, k):
    m, n = np.shape(xmat)
    ypred = np.zeros(m)
    for i in range(m):
        ypred[i] = xmat[i] * localWeight(xmat[i], xmat, ymat, k)
    return ypred
mbill = np.mat(bill)
mtip = np.mat(tip)
m = np.shape(mbill)[1]
one = np.mat(np.ones(m))
X = np.hstack((one.T, mbill.T))
ypred = localWeightRegression(X, mtip, 0.5)
SortIndex = X[:, 1].argsort(0)
xsort = X[SortIndex][:, 0]
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.scatter(bill, tip, color='orange')
ax.plot(xsort[:, 1], ypred[SortIndex], color='blue', linewidth=5)
plt.xlabel('Total Bill')
plt.ylabel('Tips')
plt.show()
