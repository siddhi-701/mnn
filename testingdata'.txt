#Practical 1
#Performing matrix multiplication and finding eigen vectors and eigen values using TensorFlow.
import tensorflow as tf
import warnings
warnings.filterwarnings('ignore')
print("Matrix Multiplication Demo")
x = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
print(x)
y = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
print(y)
z = tf.matmul(x, y)
print("Product: ", z)
e_matrix_A = tf.random.uniform([2, 2], minval=3, maxval=10, dtype=tf.float32,
name="matrixA")
print("Matrix A: \n{}\n\n".format(e_matrix_A))
eigen_values_A, eigen_vectors_A = tf.linalg.eigh(e_matrix_A)
print("Eigen Vectors: \n{}\n\n Eigen Values:\n{}\n".format(eigen_vectors_A,
eigen_values_A))


#Practical 2
#Solving XOR problem using deep feed forward network.
import numpy as np
from keras.layers import Dense
from keras.models import Sequential
model = Sequential()
model.add(Dense(units=2, activation='relu', input_dim = 2))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam',
metrics=['accuracy'])
print(model.summary())
print(model.get_weights())
X = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])
Y = np.array([0., 1., 1., 0.])
model.fit(X, Y, epochs=1000, batch_size=4)
print(model.get_weights())
print(model.predict(X, batch_size=4))


#Practical 3
#Implementing deep neural network for performing binary classification task.

import pandas as pd
from keras.models import Sequential
from keras.layers import Dense

# Load the dataset using pandas to handle the header
dataset = pd.read_csv('/content/diabetes.csv')

# Split the dataset into input (X) and output (Y)
X = dataset.iloc[:, 0:8].values
Y = dataset.iloc[:, 8].values

# Define the Sequential model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X, Y, epochs=150, batch_size=10)

# Evaluate the model
_, accuracy = model.evaluate(X, Y)
print('Accuracy of the model is:', accuracy * 100)

# Make predictions
predictions = model.predict(X)

# Display predictions for the first 5 instances
for i in range(5):
    print(f'Input: {X[i].tolist()}, Predicted: {predictions[i][0]}, Actual: {Y[i]}')


#Practical No. 4A:
#Using deep feed forward network with two layers for performing multiclass classification and predicting the class.
from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler
import numpy as np
X, Y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)
scalar = MinMaxScaler()
scalar.fit(X)
X = scalar.transform(X)
model = Sequential()
model.add(Dense(4, input_dim = 2, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit(X, Y, epochs=500)
Xnew, Yreal = make_blobs(n_samples=3, centers=2, n_features=2, random_state=1)
Xnew = scalar.transform(Xnew)
Ynew = model.predict(Xnew)
Ynew = np.round(Ynew)
for i in range(len(Xnew)):
  print("X = %s, Predicted = %s, Desired = %s" %(Xnew[i],Ynew[i],Yreal[i]))
  
  
#Practical 4B
#Using a deep feed forward network with two hidden layers for performing classification and predicting the probability of class.
from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler
import numpy as np
X, Y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)
scalar = MinMaxScaler()
scalar.fit(X)
X = scalar.transform(X)
model = Sequential()
model.add(Dense(4, input_dim = 2, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit(X, Y, epochs=500)
Xnew, Yreal = make_blobs(n_samples=3, centers=2, n_features=2, random_state=1)
Xnew = scalar.transform(Xnew)
Yclass = model.predict(Xnew)
Yclass = np.round(Yclass)
Ynew = model.predict(Xnew)
for i in range(len(Xnew)):
  print("X = %s, Predicted_probability = %s, Predicted_class = %s"
%(Xnew[i], Ynew[i], Yclass[i]))


#Practical 4C
#Using deep feed forward network with the hidden layers for performing linear regression and predicting values.
from keras.models import Sequential
from keras.layers import Dense
from sklearn.datasets import make_regression
from sklearn.preprocessing import MinMaxScaler
import numpy as np
X, Y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=1)
scalarX, scalarY = MinMaxScaler(), MinMaxScaler()
scalarX.fit(X)
scalarY.fit(Y.reshape(100, 1))
X = scalarX.transform(X)
Y = scalarY.transform(Y.reshape(100, 1))
model = Sequential()
model.add(Dense(4, input_dim=2, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='mse', optimizer='adam')
model.fit(X, Y, epochs=1000, verbose=0)
Xnew, a = make_regression(n_samples=3, n_features=2, noise=0.1,
random_state=1)
Xnew = scalarX.transform(Xnew)
Ynew = model.predict(Xnew)
for i in range(len(Xnew)):
  print("X = %s, Predicted = %s" %(Xnew[i], Ynew[i]))


#5A
# Evaluating feed forward deep network for regression using KFold cross validation.
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Load the dataset
dataframe = pd.read_csv('/content/housing (1).csv')

# Inspect the data to identify non-numeric columns
print(dataframe.dtypes)

# Handle non-numeric columns
# For example, dropping any non-numeric columns that are irrelevant
# If there are columns like addresses, drop them
dataframe = dataframe.select_dtypes(include=['number'])  # Keeping only numeric columns

# Ensure there are no missing values
dataframe.fillna(dataframe.mean(), inplace=True)

X = dataframe.iloc[:, :-1].values
Y = dataframe.iloc[:, -1].values

# Define the wider model
def wider_model():
    model = Sequential()
    model.add(Dense(15, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))
    model.add(Dense(13, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

# Create a pipeline with standardization and the Keras model
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))
pipeline = Pipeline(estimators)

# Evaluate the model using k-fold cross-validation
kfold = KFold(n_splits=10)
results = cross_val_score(pipeline, X, Y, cv=kfold)

# Print the results
print("Wider: %.2f (%.2f) MSE" % (results.mean(), results.std()))


#Practical 5B
#Evaluating feed forward deep network for multiclass Classification using KFold cross validaton
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from scikeras.wrappers import KerasClassifier
from keras import utils
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder

# Load the dataset, specifying that the first row contains headers
df = pd.read_csv('/content/flowers.csv')

# Inspect the first few rows to ensure correct data loading
print(df.head())

# Drop any non-numeric columns that might be present (e.g., headers)
# Assuming columns 0:4 are the features and column 4 is the target
X = df.iloc[:, 0:4].astype(float)  # Convert feature columns to float
Y = df.iloc[:, 4]  # Target variable

# Encode the target variable
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
print(encoded_Y)

# Convert encoded labels to categorical (one-hot encoding)
dummy_Y = utils.to_categorical(encoded_Y)
print(dummy_Y)

# Define the model
def baseline_model():
    model = Sequential()
    model.add(Dense(8, input_dim=4, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Initialize and fit the model
estimator = baseline_model()
estimator.fit(X, dummy_Y, epochs=100, shuffle=True, verbose=0)

# Make predictions
action = estimator.predict(X)

# Print the first 25 actual and predicted values
for i in range(25):
    print("Actual:", dummy_Y[i])
    print("Predicted:", action[i])
    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^')


#Practical 6A
#Implementing regularization to avoid overfitting in binary classification.
from matplotlib import pyplot as plt
from sklearn.datasets import make_moons
from keras.models import Sequential
from keras.layers import Dense
X, Y = make_moons(n_samples=100, noise=0.2, random_state=1)
n_train = 30
trainX, testX = X[:n_train,:], X[n_train:]
trainY, testY = Y[:n_train], Y[n_train:]
model = Sequential()
model.add(Dense(500, input_dim=2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam',
metrics=['accuracy'])
history = model.fit(trainX, trainY, validation_data=(testX, testY),
epochs=4000)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from sklearn.preprocessing import MinMaxScaler
dataset_train = pd.read_csv('/content/Google_Stock_Price_Train.csv')
training_set = dataset_train.iloc[:,1:2].values
sc = MinMaxScaler(feature_range=(0,1))
training_set_scaled = sc.fit_transform(training_set)
X_train = []
Y_train = []
for i in range(60, 1258):
  X_train.append(training_set_scaled[i-60:i,0])
  Y_train.append(training_set_scaled[i,0])
X_train, Y_train = np.array(X_train), np.array(Y_train)
print(X_train)
print("**************************************")
print(Y_train)
print("**************************************")
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
print("**************************************")
print(X_train)
regressor = Sequential()
regressor.add(LSTM(units=50, return_sequences=True,
input_shape=(X_train.shape[1], 1)))
regressor.add(Dropout(0.2))
regressor.add(LSTM(units=50, return_sequences=True))
regressor.add(Dropout(0.2))
regressor.add(LSTM(units=50, return_sequences=True))
regressor.add(Dropout(0.2))
regressor.add(LSTM(units=50))
regressor.add(Dropout(0.2))
regressor.add(Dense(units=1))
regressor.compile(optimizer='adam', loss='mean_squared_error')
regressor.fit(X_train, Y_train, epochs=100, batch_size=32)
dataset_test = pd.read_csv('/content/Google_Stock_Price_Test.csv')
real_stock_price = dataset_test.iloc[:,1:2].values
dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']),
axis=0)
inputs = dataset_total[len(dataset_total)-len(dataset_test)-60:].values
inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)
X_test = []
for i in range(60, 80):
  X_test.append(inputs[i-60:i,0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
predicted_stock_price = regressor.predict(X_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)
plt.plot(real_stock_price, color='red', label='real google stock price')
plt.plot(predicted_stock_price, color='blue', label='predicted stock price')
plt.xlabel('time')
plt.ylabel('google stock price')
plt.legend()
plt.show()

#Practical 8
#Performing encoding and decoding of images using deep autoencoder.
import keras
from keras import layers
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
encoding_dim = 32
input_img = keras.Input(shape=(784,))
encoded = layers.Dense(encoding_dim, activation='relu')(input_img)
decoded = layers.Dense(784, activation='sigmoid')(encoded)
autoencoder = keras.Model(input_img, decoded)
encoder = keras.Model(input_img, encoded)
encoded_input = keras.Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1]
decoder = keras.Model(encoded_input, decoder_layer(encoded_input))
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
(X_train,_),(X_test,_) = mnist.load_data()
X_train = X_train.astype('float32')/255
X_test = X_test.astype('float32')/255
X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))
X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))
print(X_train.shape)
print(X_test.shape)
autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True,
validation_data=(X_test, X_test))
encoded_imgs = encoder.predict(X_test)
decoded_imgs = decoder.predict(encoded_imgs)
n = 10
plt.figure(figsize=(40, 4))
for i in range(10):
  ax = plt.subplot(3, 20, i + 1)
  plt.imshow(X_test[i].reshape(28, 28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  ax = plt.subplot(3, 20, i + 1 + 20)
  plt.imshow(encoded_imgs[i].reshape(8, 4))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  ax = plt.subplot(3, 20, 2 * 20 + i + 1)
  plt.imshow(decoded_imgs[i].reshape(28, 28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
plt.show()


#Practical 9
#Implementation of convolutional neural network to predict numbers from number images.
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten
import matplotlib.pyplot as plt
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
plt.imshow(X_train[0])
plt.show()
print(X_train[0].shape)
X_train = X_train.reshape(60000, 28, 28, 1)
X_test = X_test.reshape(10000, 28, 28, 1)
Y_train = to_categorical(Y_train)
Y_test = to_categorical(Y_test)
Y_train[0]
print(Y_train[0])
model = Sequential()
model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape = (28, 28, 1)))
model.add(Conv2D(32, kernel_size=3, activation='relu'))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3)
print(model.predict(X_test[:4]))
print(Y_test[:4])


#Practical 10
#Denoising of images using autoencoder.
import keras
from keras.datasets import mnist
from keras import layers
import numpy as np
from keras.callbacks import TensorBoard
import matplotlib.pyplot as plt
(X_train,_), (X_test,_) = mnist.load_data()
X_train = X_train.astype('float32')/255
X_test = X_test.astype('float32')/255
X_train = np.reshape(X_train, (len(X_train), 28, 28, 1))
X_test = np.reshape(X_test, (len(X_test), 28, 28, 1))
noise_factor = 0.5
X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0,
size=X_train.shape)
X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0,
size=X_test.shape)
X_train_noisy = np.clip(X_train_noisy, 0., 10)
X_test_noisy = np.clip(X_test_noisy, 0., 1.)
n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n + 1):
  ax = plt.subplot(1, n, i)
  plt.imshow(X_test_noisy[i].reshape(28, 28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
plt.show()
input_img = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
encoded = layers.MaxPool2D((2, 2), padding='same')(x)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(X_train_noisy, X_train, epochs=3, batch_size=128,
shuffle=True,validation_data=(X_test_noisy, X_test),
callbacks=[TensorBoard(log_dir='/tmo/tb', histogram_freq=0,
write_graph=False)])
predictions = autoencoder.predict(X_test_noisy)
m = 10
plt.figure(figsize=(20, 2))
for i in range(1, m + 1):
  ax = plt.subplot(1, m, i)
  plt.imshow(predictions[i].reshape(28, 28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
plt.show()





