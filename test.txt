Practical 1a
Steps to install python
pip install nltk

Practical 1b
pip install gtts
from gtts import gTTS
myText = "Welcome to Natural Language Processing"
language = "en"
myObj = gTTS(text=myText, lang=language, slow=False)
myObj.save("myfile.mp3")

Practical 1c
pip install SpeechRecognition
import speech_recognition as sr
filename = "male.wav"
r = sr.Recognizer()
with sr.AudioFile(filename) as source:
    audio_data = r.record(source)
    text = r.recognize_google(audio_data)
    print(text)
	
Practical 2
Brown Corpus
import nltk
nltk.download('brown')
from nltk.corpus import brown
print("File Ids of brown corpus:\n", brown.fileids())
print("\nRaws in brown corpus:\n", len(brown.raw())) 
print("\nWords in brown corpus:\n", brown.words()) 
print("\nSentences in brown corpus:\n", brown.sents()) 
print("\nCategories in brown corpus:\n", brown.categories())

Inaugural Corpus:
import nltk
nltk.download("inaugural")
from nltk.corpus import inaugural
print("File Ids of inaugural corpus:\n", inaugural.fileids())
print("\nRaws in inaugural corpus:\n", len(inaugural.raw()))
print("\nWords in inaugural corpus:\n", inaugural.words())
print("\nSentences in inaugural corpus:\n", inaugural.sents())

Reuters Corpus:
import nltk
nltk.download("reuters")
from nltk.corpus import reuters
print("Reuters corpus has", len(reuters.fileids()), "FileIds")
print("\nRaws in reuters corpus:\n", len(reuters.raw())) 
print("\nWords in reuters corpus:\n", reuters.words()) 
print("\nSentences in reuters corpus:\n", reuters.sents()) 
print("\nCategories in reuters corpus:\n", reuters.categories())

UDHR Corpus:
import nltk
nltk.download('udhr')
from nltk.corpus import udhr
print("File Ids of udhr corpus:\n", udhr.fileids()) 
print("\nRaws in udhr corpus:\n", len(udhr.raw())) 
print("\nWords in udhr corpus:\n", udhr.words()) 
print("\nSentences in udhr corpus:\n", udhr.sents())

Practical 2B: Create and Use Your Own Corpora
import nltk
nltk.download('punkt')
from nltk.corpus import PlaintextCorpusReader
corpus_root = 'NLP/Practical'
filelist = PlaintextCorpusReader(corpus_root, '.*\.py')

print('\nFile list: \n', filelist.fileids())
print(filelist.root)
print('\nStatistics for each text:\n')
print("AvgWordLen\tAvgSentenceLen\tNo.OfTimesEachWordAppearsOnAvg\t\tFileName")
for fileid in filelist.fileids():
    num_chars = len(filelist.raw(fileid))
    num_words = len(filelist.words(fileid))
    num_sents = len(filelist.sents(fileid))
    num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))
    print(int(num_chars/num_words), '\t\t\t', int(num_words/num_sents),
          '\t\t\t', int(num_words/num_vocab), '\t\t\t', fileid)

Practical 2C: Study Conditional Frequency Distributions
import nltk
from nltk.corpus import brown
genre_word = [(genre, word)
              for genre in ['news', 'romance']
              for word in brown.words(categories=genre)]

cfd = nltk.ConditionalFreqDist(genre_word)
print(cfd)
print(cfd.conditions())
print(cfd['news'])
print(cfd['romance'])

Practical 2D: Study of Tagged Corpora
import nltk
nltk.download('treebank')
from nltk.corpus import treebank
tagged_sentences = treebank.tagged_sents()
print("Tagged Sentences: ", tagged_sentences[:2])
tagged_wrds = treebank.tagged_words()[:10]
print("Tagged Words: ", tagged_wrds)

Practical  2E: Find the Most Frequent Noun Tags
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import defaultdict
text = "Nick likes to play football. Nick does not like to play cricket."
tokens = nltk.word_tokenize(text)
tagged = nltk.pos_tag(tokens)
print(tagged)
noun_words = [word for word, pos in tagged if pos.startswith('NN')]
freq_dist = defaultdict(int)
for noun in noun_words:
    freq_dist[noun] += 1
most_freq_noun = max(freq_dist, key=freq_dist.get)
print("Noun with maximum frequency: ", most_freq_noun)

Practical 2F: Map Words to Properties Using Python Dictionaries
car = {
    "brand": "Ford",
    "model": "Mustang",
    "year": 1964
}
print(car)
print("Brand:", car["brand"])
print("Dictionary length:", len(car))
print("Type of object:", type(car))

Practical 2G: Study DefaultTagger, RegexpTagger, and UnigramTagger
DefaultTagger:
import nltk
from nltk.tag import DefaultTagger
from nltk.corpus import treebank
default_tagger = DefaultTagger('NN')
test_sentences = treebank.tagged_sents()[1000:]
print(default_tagger.accuracy(test_sentences))

RegexpTagger:
from nltk.corpus import brown
from nltk.tag import RegexpTagger
test_sent = brown.sents(categories='news')[0]
regexp_tagger = RegexpTagger(
    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),
     (r'(The|the|A|a|An|an)$', 'AT'),
     (r'.*able$', 'JJ'),
     (r'.*ness$', 'NN'),
     (r'.*ly$', 'RB'),
     (r'.*s$', 'NNS'),
     (r'.*ing$', 'VBG'),
     (r'.*ed$', 'VBD'),
     (r'.*', 'NN')]
)
print(regexp_tagger.tag(test_sent))

UnigramTagger:
from nltk.tag import UnigramTagger
from nltk.corpus import treebank
train_sents = treebank.tagged_sents()[:10]
unigram_tagger = UnigramTagger(train_sents)
print(treebank.sents()[0])
print(unigram_tagger.tag(treebank.sents()[0]))

Practical  2H: Word Segmentation and Scoring
import re
words = ["what", "is", "my", "name"]
text = "whatismyname"
segmented_words = []
i = 0
while i < len(text):
    for word in words:
        if text.startswith(word, i):
            segmented_words.append(word)
            i += len(word)
            break
segmented_text = ' '.join(segmented_words)
print("Segmented Text:", segmented_text)
score = len(segmented_words) * 10  # Example scoring
print("Score:", score)

Practical 3A Study of Wordnet Dictionary with methods as synsets, definitions, examples, antonyms.
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
print("Synsets for 'computer':", wordnet.synsets("computer"))
print("Definition of 'computer.n.01':", wordnet.synset("computer.n.01").definition())
print("Examples of 'computer.n.01':", wordnet.synset("computer.n.01").examples())
antonyms = wordnet.lemma('buy.v.01.buy').antonyms()
print("Antonyms of 'buy.v.01.buy':", [antonym.name() for antonym in antonyms] if antonyms else "No antonyms found")

Practical  3B: Study lemmas, hyponyms, hypernyms, entailments
import nltk
from nltk.corpus import wordnet
nltk.download('wordnet')
synsets = wordnet.synsets("computer")
print("Synsets for 'computer':", synsets)
print("\nLemma names for 'computer.n.01':", wordnet.synset("computer.n.01").lemma_names())
print("\nAll lemma names for synsets of 'computer':")
for e in synsets:
    print(f'{e} --> {e.lemma_names()}')
print("\nLemmas for 'computer.n.01':", wordnet.synset('computer.n.01').lemmas())
lemma = wordnet.lemma('computer.n.01.computing_device')
print("\nSynset for lemma 'computing_device':", lemma.synset())
print("Lemma name:", lemma.name())
syn = wordnet.synset('computer.n.01')
print("\nHyponyms of 'computer.n.01':", syn.hyponyms())
print("\nLemma names for hyponyms of 'computer.n.01':",
      [lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])
vehicle = wordnet.synset('vehicle.n.01')
car = wordnet.synset('car.n.01')
print("\nLowest common hypernym of 'vehicle.n.01' and 'car.n.01':", car.lowest_common_hypernyms(vehicle))

Practical 3C: Find Synonyms and Antonyms of the word "active" using WordNet
from nltk.corpus import wordnet
nltk.download('wordnet')
synonyms = []
for syn in wordnet.synsets("active"):
    for lemma in syn.lemmas():
        synonyms.append(lemma.name())
synonyms = set(synonyms)
print("Synonyms of the word 'active':", synonyms)
antonyms = []
for syn in wordnet.synsets("active"):
    for lemma in syn.lemmas():
        if lemma.antonyms():
            antonyms.append(lemma.antonyms()[0].name())
antonyms = set(antonyms)
print("Antonyms of the word 'active':", antonyms)

Practical 3D: Compare Two Nouns	
import nltk
from nltk.corpus import wordnet
nltk.download('wordnet')
syn1 = wordnet.synsets('football')
syn2 = wordnet.synsets('soccer')
for s1 in syn1:
    for s2 in syn2:
        print("Path similarity of:")
        print(f"{s1} ({s1.pos()}) [{s1.definition()}]")
        print(f"{s2} ({s2.pos()}) [{s2.definition()}]")
        print("Similarity score:", s1.path_similarity(s2))
        print()

Practical 3E: Handling Stop Words
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
text = "Yashesh likes to play football, however he is not too fond of tennis."
text_tokens = word_tokenize(text)
tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]
print("Without stopwords:", tokens_without_sw)
all_stopwords = stopwords.words('english')
all_stopwords.append('play')
tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]
print("Without stopwords (with 'play' added):", tokens_without_sw)
all_stopwords.remove('not')
tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]
print("Without stopwords (with 'not' removed):", tokens_without_sw)


Practical 4A: Tokenization using Python’s split() function
text = """This tool is in a beta stage. Alexa developers can use Get Metrics API to seamlessly analyse metrics. It also supports custom skill model, prebuilt Flash Briefing model, and the Smart Home Skill API. You can use this tool for creation of monitors, alarms, and dashboards that spotlight changes. The release of these three tools will enable developers to create visually rich skills for Alexa devices with screens. Amazon describes these tools as the collection of tech and tools for creating visually rich and interactive voice experiences."""
data = text.split('.')
for i in data:
    print(i.strip())

Practical4B: Tokenization using Regular Expressions (RegEx)
import nltk
from nltk.tokenize import RegexpTokenizer
tk = RegexpTokenizer('\s+', gaps=True)
str = "I love to study Natural Language Processing in Python"
tokens = tk.tokenize(str)
print(tokens)

 4C: Tokenization using NLTK
 import nltk
from nltk.tokenize import word_tokenize
str = "I love to study Natural Language Processing in Python"
tokens = word_tokenize(str)
print(tokens)

4D: Tokenization using the Spacy library
import spacy
nlp = spacy.blank("en")
str = "I love to study Natural Language Processing in Python"
doc = nlp(str)
words = [word.text for word in doc]
print(words)

 4E: Tokenization using Keras
 pip install keras-preprocessing
from keras_preprocessing.text import text_to_word_sequence
str = "I love to study Natural Language Processing in Python"
tokens = text_to_word_sequence(str)
print(tokens)

4F: Tokenization using Gensim
from gensim.utils import tokenize
str = "I love to study Natural Language Processing in Python"
tokens = list(tokenize(str))
print(tokens)

Practical 5A: Word Tokenization in Hindi
from inltk.inltk import setup, tokenize
setup('hi')
hindi_text = """प्राकृ तिक  भाषा  सीखना  बहुि  दिलचस्प  है।"""
tokens = tokenize(hindi_text, "hi")
print(tokens)

Practical 5B: Generate Similar Sentences from Hindi Input
from inltk.inltk import setup, get_similar_sentences
setup('hi')
output = get_similar_sentences("मैं आज बहुत खुश हूँ।", 5, 'hi')
print(output)

Practical 5C: Identify the Indian Language of a Text
from inltk.inltk import setup, identify_language
setup('gu')
language = identify_language("બીના કાપડિયા+")
print(language)

Practical 6A: Part of Speech Tagging and Chunking
import nltk
from nltk import tokenize, chunk
nltk.download('maxent_ne_chunker')
nltk.download('words')
para = input("Enter some text: ")
sents = tokenize.sent_tokenize(para)
print("\nSentence Tokenization:\n", sents)
for index in range(len(sents)):
    words = tokenize.word_tokenize(sents[index])
    print("\nWord Tokenization:\n", words)
tagged_words = []
for index in range(len(sents)):
    tagged_words.append(nltk.pos_tag(words))
print("\nPOS Tagging:\n", tagged_words)
tree = []
for index in range(len(sents)):
    tree.append(chunk.ne_chunk(tagged_words[index]))
print("\nChunking:\n", tree)

Practical 6B: Named Entity Recognition with SpaCy
import spacy
nlp = spacy.load("en_core_web_sm")
text = ("When Sebastian Thrun started working on self-driving cars at Google in 2007...")
doc = nlp(text)
print("Noun phrases: ", [chunk.text for chunk in doc.noun_chunks])
print("Verbs: ", [token.lemma_ for token in doc if token.pos_ == "VERB"])

Practical 6C: Named Entity Recognition with NLTK’s Treebank Corpus

import nltk
nltk.download('treebank')
from nltk.corpus import treebank_chunk
treebank_chunk.chunked_sents()[0].draw()

Practical 7A: Defining Grammar Using NLTK and Analyzing a Sentence
import nltk
from nltk import tokenize
grammar1 = nltk.CFG.fromstring("""
S -> VP
VP -> 'Book' NP
NP -> Det NP | 'flight'
Det -> 'that'
""")
sentence = "Book that flight"
all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)
parser = nltk.ChartParser(grammar1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()

Practical 7B: Finite Automata - Regular Expression of FA: 101+
def FA(s):
    if len(s) < 3: return "Rejected"
    if s[0] == '1' and s[1] == '0' and s[2] == '1':
        for i in range(3, len(s)):
            if s[i] != '1':
                return "Rejected"
        return "Accepted"
    return "Rejected"
inputs = ['1', '10101', '101', '10111', '01010', '100', '', '10111101', '1011111']
for i in inputs:
    print(FA(i))

Practical 7C: Finite Automata - Regular Expression of FA: (a+b)*bba
def FA(s):
    size = 0
    for i in s:
        if i in {'a', 'b'}: size += 1
        else: return "Rejected"
    if size >= 3 and s[size-3:] == "bba":
        return "Accepted"
    return "Rejected"
inputs = ['bba', 'ababbba', 'abba', 'abb', 'baba', 'bbb', '']
for i in inputs:
    print(FA(i))

Practical 7D: Deductive Chart Parsing Using CFG
import nltk
from nltk import tokenize
grammar1 = nltk.CFG.fromstring("""
S -> NP VP
VP -> V NP | VP PP
PP -> P NP
NP -> Det N | Det N PP | 'I'
Det -> 'a' | 'my'
N -> 'bird' | 'balcony'
V -> 'saw'
P -> 'in'
""")
sentence = "I saw a bird in my balcony"
all_tokens = tokenize.word_tokenize(sentence)
print(all_tokens)
parser = nltk.ChartParser(grammar1)
for tree in parser.parse(all_tokens):
    print(tree)
    tree.draw()
	
Practical 8: Stemming and Lemmatization

PorterStemmer: Reduces words to their root form using the Porter Stemming algorithm.
from nltk.stem import PorterStemmer
word_stemmer = PorterStemmer()
print("PorterStemmer:", word_stemmer.stem('writing'))
LancasterStemmer: Another stemming algorithm that's more aggressive than PorterStemmer.
 
from nltk.stem import LancasterStemmer
lanc_stemmer = LancasterStemmer()
print("LancasterStemmer:", lanc_stemmer.stem('writing'))

RegexpStemmer: Stemming based on regular expressions.
from nltk.stem import RegexpStemmer
reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)
print("RegexpStemmer:", reg_stemmer.stem('writing'))

SnowballStemmer: A more advanced stemming algorithm with support for multiple languages.
from nltk.stem import SnowballStemmer
english_stemmer = SnowballStemmer('english')
print("SnowballStemmer:", english_stemmer.stem('writing'))

WordNetLemmatizer: Converts words to their base form using the WordNet database.
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print("\nWordNetLemmatizer:")
print("rocks:", lemmatizer.lemmatize("rocks"))
print("corpora:", lemmatizer.lemmatize("corpora"))
print("better:", lemmatizer.lemmatize("better", pos="a"))

Practical 9A: Naive Bayes Classifier Implementation
from sklearn.naive_bayes import MultinomialNB
multinomialnb = MultinomialNB()
multinomialnb.fit(x_train, y_train)
y_pred = multinomialnb.predict(x_test)

Practical 10A: Speech Tagging
Using SpaCy:
SpaCy's en_core_web_sm model is used for part-of-speech (POS) tagging.
displacy is used to visualize dependency trees. 
import spacy
sp = spacy.load('en_core_web_sm')
sen = sp(u"I like to play football.")
for word in sen:
    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')

Using NLTK:
NLTK's PunktSentenceTokenizer and POS tagger are used for speech tagging. 
import nltk
from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer
custom_sent_tokenizer = PunktSentenceTokenizer(train_text)
tokenized = custom_sent_tokenizer.tokenize(sample_text)
def process_content():
    for i in tokenized[:2]:
        words = nltk.word_tokenize(i)
        tagged = nltk.pos_tag(words)
        print(tagged)
process_content()

Practical 10B: Statistical Parsing 
import nltk
def give(t):
    return t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP' and ('give' in t[0].leaves() or 'gave' in t[0].leaves())
for tree in nltk.corpus.treebank.parsed_sents():
    for t in tree.subtrees(give):
        print_node(t, 72)
		
Probabilistic Parsing: 
from nltk import PCFG
grammar = PCFG.fromstring('''
NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]
NNS -> "men" [0.1] | "women" [0.2] | "children" [0.3] | NNS CC NNS [0.4]
JJ -> "old" [0.4] | "young" [0.6]
CC -> "and" [0.9] | "or" [0.1]
''')
viterbi_parser = nltk.ViterbiParser(grammar)
token = "old men and women".split()
obj = viterbi_parser.parse(token)
for x in obj:
    print(x)
	
Practical 10C: Malt Parsing
Download Required Files:
maltparser-1.7.2.zip (MaltParser software)
engmalt.linear-1.7.mco (English model for MaltParser)
Extract and Place Files:
Extract maltparser-1.7.2.zip to C:\Users\UserName\AppData\Local\Programs\Python\Python312.
Copy engmalt.linear-1.7.mco to C:\Users\UserName\AppData\Local\Programs\Python\Python312.
Set Environment Variables:
Open "Edit the system environment variables" from the Start Menu.
Click on "Environment Variables".
Add new environment variables:
MALT_PARSER: C:\Users\UserName\AppData\Local\Programs\Python\Python312\maltparser-1.7.2
MALT_MODEL: C:\Users\UserName\AppData\Local\Programs\Python\Python312\engmalt.linear-1.7.mco

from nltk.parse import malt
mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco')
t = mp.parse_one("I saw a bird from my window.".split()).tree()
print(t)
t.draw()

Practical No. 11A: Multiword Expressions Using MWETokenizer: Handles tokenization of multiword expressions.
from nltk.tokenize import MWETokenizer, sent_tokenize, word_tokenize
s = '''Good cake cost RS.1500/kg in Mumbai. Please buy me one of them.\n\nThanks.'''
mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')

for sent in sent_tokenize(s):
    print(mwe.tokenize(word_tokenize(sent)))
	
Practical No. 11B: Text Normalization and Clustering Normalize Texts and Group Using Clustering:
import numpy as np
import re
import textdistance
from sklearn.cluster import AgglomerativeClustering
texts = [
    'Reliance Supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Reliance market',
    'Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport',
    'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading'
]
def normalize(text):
    return re.sub('[^a-z0-9]+', '', text.lower())
def group_texts(texts, threshold=0.4):
    normalized_texts = np.array([normalize(text) for text in texts])
    distances = 1 - np.array([
        [textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts
    ])
    clustering = AgglomerativeClustering(
        distance_threshold=threshold, metric="precomputed",
        linkage="complete", n_clusters=None
    ).fit(distances)
    centers = dict()
    for cluster_id in set(clustering.labels_):
        index = clustering.labels_ == cluster_id
        centrality = distances[:, index][index].sum(axis=1)
        centers[cluster_id] = normalized_texts[index][centrality.argmin()]
    return [centers[i] for i in clustering.labels_]
print(group_texts(texts))

Practical No. 11C: WordNet Synsets  Get the Best Synset for a Word
from nltk.corpus import wordnet as wn
def get_first_sense(word, pos=None):
    if pos:
        synsets = wn.synsets(word, pos)
    else:
        synsets = wn.synsets(word)
    return synsets[0] if synsets else None
best_synset = get_first_sense('bank')
print('%s: %s' % (best_synset.name(), best_synset.definition()) if best_synset else "No synset found")
best_synset = get_first_sense('set', 'n')
print('%s: %s' % (best_synset.name(), best_synset.definition()) if best_synset else "No synset found")
best_synset = get_first_sense('set', 'v')
print('%s: %s' % (best_synset.name(), best_synset.definition()) if best_synset else "No synset found")
